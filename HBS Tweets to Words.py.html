#!/usr/bin/env python
# coding: utf-8

# In[1]:


# Libraries
import pandas as pd
import nltk
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image


# In[43]:


import nltk
nltk.download('punkt')


# In[2]:


## Load in Tweets
df = pd.read_csv('HarvardHBS.csv')


# In[3]:


# Tokenize words
texts = ' '.join(df['Text'])


# In[4]:


FindWordsText = nltk.word_tokenize(texts)


# In[5]:


text = nltk.Text(FindWordsText)


# In[6]:


# Spit out finished, searchable version of text
words = text.tokens


# In[7]:


# Clean up links as words
words_clean = [i for i in words if '//' not in i]
words_clean = [i for i in words_clean if '.com' not in i]
other_messy_words = ['https', ' https', 'https ', '.',':','\'','&',',']
words_clean = [i for i in words_clean if i not in other_messy_words]


# In[8]:


print(len(words))
print(len(words_clean))


# In[9]:


text_clean = pd.Series(words_clean)
text_clean.to_csv('HBSCleanWords.csv')


# In[10]:


# Get text_clean in string again
texts_clean = ' '.join(words_clean)


# In[11]:


# Redo NLTK
FindWordsText = nltk.word_tokenize(texts_clean)


# In[12]:


print(list(FindWordsText))


# In[13]:


#Seperate text into words and preprocessing
text = nltk.Text(FindWordsText) # Spit out finished, searchable version of text
words = text.tokens


# In[14]:


# Analyze word
def analyze_word_frequency(words,num_to_show,save_name,color):
    frequency_dist = nltk.FreqDist(words)
    word_dict = sorted((value, key) for (key,value) in frequency_dist.items())
    x, y = zip(*word_dict) # unpack a list of pairs into two tuples
    plt.figure(figsize=[6,14])
    plt.plot(x[-num_to_show:], y[-num_to_show:],lw=6,c=color)
    plt.yticks(fontname = "Arial Black", fontsize=20)
    plt.xlabel('Num Times Used',fontsize=14, fontname = "Arial Black")
    plt.xticks(fontsize=14, fontname = "Arial Black")
    plt.tight_layout()
    plt.savefig(save_name + ".png", format="png")
    plt.show()


# In[15]:


# Analyze Frequency 
analyze_word_frequency(words,50,'HBS_most_common','r')


# In[16]:


nltk.download('stopwords')


# In[17]:


stop_words = set(nltk.corpus.stopwords.words('english')) 
filtered_by_stopwords = [w for w in FindWordsText if not w in stop_words] 
extra_stopwords = ["…",'#','@','!','\'s','..',"’",'?','n\'t','.','I','...']
filtered_by_stopwords = [w for w in filtered_by_stopwords if not w in extra_stopwords] 


# In[18]:


text_filtered = pd.Series(filtered_by_stopwords)
text_filtered.to_csv('HBSFilteredWords.csv')


# In[19]:


len(text_filtered)


# In[ ]:





# In[20]:


# Redo frequency analysis
analyze_word_frequency(filtered_by_stopwords,50,'HBS_most_common_clean','b')


# In[21]:


print(stop_words)


# In[22]:


print(len(filtered_by_stopwords))


# In[23]:


### Make Word Cloud


# In[24]:


# Simple word cloud


# In[25]:


# Create and generate a word cloud image:
wordcloud = WordCloud(background_color="white").generate(texts_clean)


# In[26]:


# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()
plt.figure(figsize=[12,12])
wordcloud.to_file("HBS_simple_tweets.png")


# In[27]:


#make image word bubble


# In[28]:


mask = np.array(Image.open("HBSLibrary.jpg"))
wordcloud_colored = WordCloud( background_color="white",  mask=mask).generate(texts_clean)


# In[29]:


# create coloring from image
image_colors = ImageColorGenerator(mask)
plt.figure(figsize=[12,12])
plt.imshow(wordcloud_colored.recolor(color_func=image_colors), interpolation="bilinear")
plt.axis("off")


# In[ ]:





# In[ ]:




